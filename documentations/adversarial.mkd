## Related work
### 1. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations by MIT ppl workshop paper
[https://arxiv.org/pdf/1712.02779.pdf]
- Shows that simple rotations (-30 to +30 deg)or translations(10% of the image pixels) or both are enough to reduce the accuracy of NN
For the standard
models, accuracy drops from 99% to 26% on MNIST, 93%
to 3% on CIFAR10, and 76% to 31% on ImageNet

- Dataset: MNIST, CIFAR10, Imagenet
- Network architecture: Standard from tf tutorial for MNIST.
- Black box adversarial training technique
- Paper proposes to use worst of 10 adversary i.e. worse of 10 perturbations and train the network by adding this perturbation.
- #### Solution proposed: randomly sampling a few transformations of
each training point and training on the worst significantly
improves the modelâ€™s performance. Furthermore, by making
predictions based on a majority vote of randomly perturbed
versions of the input, we can further increase classification
accuracy in the adversarial setting.
- #### Criticisms: Does not try the adversarial training models or STNs
  - does not look into better/robust network architectures.

### 2. SYNTHESIZING ROBUST ADVERSARIAL EXAMPLES: Reject from ICLR 2018, potential future paper.
[https://arxiv.org/pdf/1707.07397.pdf]
- implementation: [https://github.com/prabhant/synthesizing-robust-adversarial-examples]
- #### Idea: Produce adversarial images that are robust to noise, distortion, and affine transformation 
- Contribution:  
-- Developed Expectation Over Transformation (EOT), a novel algorithm that produces single adversarial examples that are simultaneously adversarial over an entire distribution of transformations
-- show the same for  3d objects, showing that this is of real concern for real world NNs.

### 3. Spatially Transformed adversarial examples - ICLR 18 by UCB and UMich
[https://arxiv.org/pdf/1801.02612.pdf]
- A novel white box adversarial attack for generating spatially transformed adversarial images.
- Dataset: MNIST, CIFAR10, Imagenet
- Network: standard CNNs, adversarial training, ensemble training
- All the existing approaches directly modify pixel values, which may sometimes produce noticeable
artifacts. Instead, this paper aims to smoothly change the geometry of the scene while keeping the original
appearance, producing more perceptually realistic adversarial examples.
- ![method](https://github.com/GaoGroupUCSD/STN-exp/blob/master/images/adversarial/e4.PNG)
- The function is minimizing 2 terms where L_adv encourages the generated adversarial examples to be misclassified by the target classifier. L_flow ensures that the spatial transformation distance is minimized to preserve high perceptual
quality, and tau balances these two losses. 
![equation](https://github.com/GaoGroupUCSD/STN-exp/blob/master/images/adversarial/e1.PNG)
-  L_adv i.e. the maximum distance of the target from the other classes. here k is the confidence. Paper used k=0 
![equation](https://github.com/GaoGroupUCSD/STN-exp/blob/master/images/adversarial/e2.PNG)
- L_flow i.e. smoothness penalty to make sure that the transformed pixels are not too different from its 4 neighbours.
![equation](https://github.com/GaoGroupUCSD/STN-exp/blob/master/images/adversarial/e3.PNG)
- Attack success rate is ~99% on MNIST.

