# Transformations of convolutional filters 

## Idea 
To transform the filters according to the input /part of the image we are convolving on at a time.

## Goal:
- decrease the number of filters, 
- increase in accuracy

## Related work:
### 1. Dynamic Steerable blocks in Deep Residual Networks
[https://arxiv.org/pdf/1706.00598.pdf]
- Dataset:  Berkeley Segmentation contour detection dataset
-  able to seamlessly
transform filters under pre-defined transformations, conditioned on the input at
training and inference time.
- Dynamic steerable blocks learn the degree of invariance
from data and locally adapt filters, allowing them to apply a different geometrical variant
of the same filter to each location of the feature map. 

### 2. A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations (By MIT Feb 2018)
[https://arxiv.org/pdf/1712.02779.pdf]
- Paper is based on the idea that it is considerably easy to fool a CNN network by small transformations like rotations (-30 to +30 degrees), translation(10% of the image)  and scaling. These transformations are very easy to be classified when seen a human.
- Dataset: MNIST, CIFAR10, ImageNet
- To choose the transformations they claim that they dont need to know the exact model but  choosing the worst out
of 10 random transformations is sufficient to reduce the
accuracy of these models by 26% on MNIST, 72% on CIFAR10,
and 28% on ImageNet (Top 1)
- Recall that, in the context of vision models, an adversarial example for a given image x and a classifier C is an image x_0 that, on one hand, causes the classifier C to output a different label on x_0 than on x, i.e., to have C(x) != C(x_0), but, on the other hand, is “visually similar” to x.
- Most works on visual similarity assume that Arguably, when two images are close in some `p` norm they are visually similar. 
- Given that we cannot fully optimize over the space of transformations, we use a coarse approximation provided by the
worst-of-10 adversary (described in Section 2). That is, first
we sample 10 random transformations of each training example
uniformly from the space of allowed transformations.
We then evaluate the model on each of these transformations
and choose to train on the one with highest loss. This
corresponds to minimizing a min-max formulation of the
problem similar to that in (Madry et al., 2017). Training
against such an adversary increases the training by a factor
of roughly six
- Results ![results](https://github.com/GaoGroupUCSD/STN-exp/blob/master/images/adversarial_network1.PNG)
#### Idea : Create the worst perturbed dataset, on which the network is performing the worst and then train on that. 

### 3. Generalizing Pooling Functions in CNNs: Mixed, Gated, and Tree (By Prof Zhuowen grp)
[http://pages.ucsd.edu/~ztu/publication/pami_gpooling.pdf]
- The two primary directions lie in: (1) learning a pooling function via (two strategies of) combining of max and average
pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned.
- 


## Experiments:
### Task:
Digit classification

### Network setup: 
<pre>
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 60, 60, 32)        288       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         
_________________________________________________________________
convolution2d_8_1 (Convoluti (None, 30, 30, 32)        9216      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         
_________________________________________________________________
convolution2d_8_2 (Convoluti (None, 15, 15, 32)        9216      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 7, 7, 32)          0         
_________________________________________________________________
convolution2d_8_3 (Convoluti (None, 5, 5, 32)          9216      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 2, 2, 32)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               33024     
_________________________________________________________________
activation_1 (Activation)    (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570      
_________________________________________________________________
activation_2 (Activation)    (None, 10)                0         
=================================================================
Total params: 63,530
Trainable params: 63,530
Non-trainable params: 0
_________________________________________________________________
</pre>
#### Baseline:
- 4 CNN layers followed by maxpool
- 2 Dense layers (256 and 10 neurons)
- Dropout: 0.3 rate
- Epochs: 110
- Optimizer: Adam

### Dataset 
Cluttered MNIST: [https://github.com/MasazI/Spatial_Transformer_Network]
Dataser: The translated and
cluttered dataset (TC) is generated by placing an MNIST digit in a random location in a 60 × 60
black canvas, and then inserting six randomly sampled 6 × 6 patches of other digit images into
random locations in the image.

Though the STN paper mentions this dataset, it does not report any results on this dataset.

### Results
#### Experiment 1: 
#### Setup:
No of filters in each convolutional layer: 32
Rotated filter layer: Convolutional layer where every filter is rotated to 45 degrees and stacked together. The output of this layer is the element wise maximum of the convolution output.

| Model  | Validation accuracy | Test accuracy| Validation loss | Test loss
| ------------- | ------------- |-------------|-----------------|-----------|
|  Baseline  | 95.1  | 95.08 | 0.35393| 0.355747|
| All 4 conv layers with 4 rotated filters  | 89.06  | 89.35| 0.7768204 |0.7627| 
| All 4 conv layers with 3 rotated filters  | 93.9  | 93.988| 0.3137442 | 0.317178| 
| All 4 conv layers with 2 rotated filters  | 94.5  | 94.47| 0.33467211 |0.3243| 
| All 4 conv layers with 1 rotated filters  | 95.1  | 95.17|0.34857| 0.3810| 

#### Experiment 2: 
#### Setup:
No of filters in each convolutional layer: 16
Rotated filter layer: Convolutional layer where every filter is rotated to 90 degrees and stacked together. The output of this layer is the element wise maximum of the convolution output.

| Model  | Validation accuracy | Test accuracy| Validation loss | Test loss
| ------------- | ------------- |-------------|-----------------|-----------|
|  Baseline  | 95.1  | 95.08 | 0.35393| 0.355747|
| All 4 conv layers with 4 rotated filters  | 91.84 | 90.84 |0.6765 | 0.6759| 
| All 4 conv layers with 3 rotated filters  | 94.5  | 94.3| 0.3489| 0.36120| 
| All 4 conv layers with 2 rotated filters  | 94.4  | 94.49| 0.35597| 0.353036| 
| All 4 conv layers with 1 rotated filters  | 95.3 | 95.25| 0.34364 | 0.3762| 


#### Experiment 3: 
#### Setup:
- No of filters in each convolutional layer: 16
- Rotated filter layer: Convolutional layer where every filter is rotated to 45 degrees and stacked together. The output of this layer is the element wise maximum of the convolution output.

| Model  | Validation accuracy | Test accuracy| Validation loss | Test loss
| ------------- | ------------- |-------------|-----------------|-----------|
|  Baseline  | 92.92  | 92.79 | 0.3429| 0.3883|
| All 4 conv layers with 3 rotated filters  | 91.5  | 91.1| 0.3484 | 0.36384| 
| All 4 conv layers with 2 rotated filters  | 91.18  | 91.32| 0.34453 | 0.3548| 
| All 4 conv layers with 1 rotated filters  | 91.53  | 91.96| 0.543602 | 0.544| 

#### Experiment 4: 
#### Setup:
No of filters in each convolutional layer: 16
Rotated filter layer: Convolutional layer where every filter is rotated to 90 degrees and stacked together. The output of this layer is the element wise maximum of the convolution output.

| Model  | Validation accuracy | Test accuracy| Validation loss | Test loss
| ------------- | ------------- |-------------|-----------------|-----------|
|  Baseline  | 93.02  | 93.25 | 0.34501| 0.33128|
| All 4 conv layers with 3 rotated filters  | 92.8  | 93.08| 0.261119 | 0.252387| 
| All 4 conv layers with 2 rotated filters  | 92.5  | 92.09| 0.30970 | 0.3239| 
| All 4 conv layers with 1 rotated filters  | 92.9  | 92.95| 0.3904366 | 0.36607| 

#### Experiment 5:
Concat all the feature maps together and backpropagate the gradients in one filter.

| Model  | Validation accuracy | Test accuracy| Validation loss | Test loss
| ------------- | ------------- |-------------|-----------------|-----------|
|  Baseline  | 95.12  | 95.25 | 0.34501| 0.33128|
| All 4 conv layers with 1 rotated filters(8 rotations)  | 95.24  | 95.19| 0.352945 | 0.360047|
| All 4 conv layers with 1 rotated filters(4 rotations)  | 95.13  | 94.93|  0.3505667 | 0.3569884|


### Findings
- If we use a rotated filter layer as the first convlolutional layer, the accuracy drop significantly to 3-4 points compared to the baseline
- For all the other layers, the accuracy is not very different when compared with applying the rotational convolutional layer to deeper layers.
- The accuracy with lesser rotational layers is the highest though with a marginal difference.
- The accuracy for 4 rotations is better in case of 16 filters as compared to 8 rotations.


### References
[https://github.com/kevinzakka/spatial-transformer-network]
