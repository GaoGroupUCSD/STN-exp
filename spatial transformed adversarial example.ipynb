{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatially transformed adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ipdb\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MNIST_data = './MNIST-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## setup_mnist.py -- mnist data and model loading code\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import urllib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(num_images*28*28)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = (data / 255) - 0.5\n",
    "        data = data.reshape(num_images, 28, 28, 1)\n",
    "        return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8)\n",
    "    return (np.arange(10) == labels[:, None]).astype(np.float32)\n",
    "\n",
    "class MNIST:\n",
    "    def __init__(self):\n",
    "        if not os.path.exists(MNIST_data):\n",
    "            os.mkdir(MNIST_data)\n",
    "            files = [\"train-images-idx3-ubyte.gz\",\n",
    "                     \"t10k-images-idx3-ubyte.gz\",\n",
    "                     \"train-labels-idx1-ubyte.gz\",\n",
    "                     \"t10k-labels-idx1-ubyte.gz\"]\n",
    "            for name in files:\n",
    "\n",
    "                urllib.urlretrieve('http://yann.lecun.com/exdb/mnist/' + name, \"MNIST_data/\"+name)\n",
    "\n",
    "        train_data = extract_data(MNIST_data + \"//train-images-idx3-ubyte.gz\", 60000)\n",
    "        train_labels = extract_labels(MNIST_data + \"/train-labels-idx1-ubyte.gz\", 60000)\n",
    "        self.test_data = extract_data(MNIST_data + \"/t10k-images-idx3-ubyte.gz\", 10000)\n",
    "        self.test_labels = extract_labels(MNIST_data + \"/t10k-labels-idx1-ubyte.gz\", 10000)\n",
    "        \n",
    "        VALIDATION_SIZE = 5000\n",
    "        \n",
    "        self.validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "        self.validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "        self.train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "        self.train_labels = train_labels[VALIDATION_SIZE:]\n",
    "\n",
    "\n",
    "class MNISTModel:\n",
    "    def __init__(self, restore, session=None):\n",
    "        self.num_channels = 1\n",
    "        self.image_size = 28\n",
    "        self.num_labels = 10\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3),\n",
    "                         input_shape=(28, 28, 1)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(32, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(64, (3, 3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(10))\n",
    "        # Behold no softmax\n",
    "        model.load_weights(restore)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, data):\n",
    "        return self.model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## train_models.py -- train the neural network models for attacking\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import tensorflow as tf\n",
    "# from setup_mnist import MNIST\n",
    "# from setup_cifar import CIFAR\n",
    "import os\n",
    "\n",
    "def train(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):\n",
    "    \"\"\"\n",
    "    Standard neural network training procedure.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    print(data.train_data.shape)\n",
    "    \n",
    "    model.add(Conv2D(params[0], (3, 3),\n",
    "                            input_shape=data.train_data.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(params[1], (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(params[2], (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(params[3], (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(params[4]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(params[5]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10))\n",
    "    # no softmax\n",
    "    if init != None:\n",
    "        model.load_weights(init)\n",
    "\n",
    "    def fn(correct, predicted):\n",
    "        return tf.nn.softmax_cross_entropy_with_logits(labels=correct,\n",
    "                                                       logits=predicted/train_temp)\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    model.compile(loss=fn,\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(data.train_data, data.train_labels,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(data.validation_data, data.validation_labels),\n",
    "              epochs=num_epochs,\n",
    "              shuffle=True)\n",
    "    \n",
    "\n",
    "    if file_name != None:\n",
    "        model.save(file_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_distillation(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1):\n",
    "    \"\"\"\n",
    "    Train a network using defensive distillation.\n",
    "    Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks\n",
    "    Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, Ananthram Swami\n",
    "    IEEE S&P, 2016.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_name+\"_init\"):\n",
    "        # Train for one epoch to get a good starting point.\n",
    "        train(data, file_name+\"_init\", params, 1, batch_size)\n",
    "    \n",
    "    # now train the teacher at the given temperature\n",
    "    teacher = train(data, file_name+\"_teacher\", params, num_epochs, batch_size, train_temp,\n",
    "                    init=file_name+\"_init\")\n",
    "\n",
    "    # evaluate the labels at temperature t\n",
    "    predicted = teacher.predict(data.train_data)\n",
    "    with tf.Session() as sess:\n",
    "        y = sess.run(tf.nn.softmax(predicted/train_temp))\n",
    "        print(y)\n",
    "        data.train_labels = y\n",
    "\n",
    "    # train the student model at temperature t\n",
    "    student = train(data, file_name, params, num_epochs, batch_size, train_temp,\n",
    "                    init=file_name+\"_init\")\n",
    "\n",
    "    # and finally we predict at temperature 1\n",
    "    predicted = student.predict(data.train_data)\n",
    "\n",
    "    print(predicted)\n",
    "    \n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# train(MNIST(), \"models/mnist_baseline\", [32, 32, 64, 64, 200, 200], num_epochs=50)\n",
    "\n",
    "# train_distillation(MNIST(), \"models/mnist-distilled-100\", [32, 32, 64, 64, 200, 200],\n",
    "#                    num_epochs=50, train_temp=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial transformed adversarial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## stAdv_attack.py -- attack a network optimizing for spatial distance\n",
    "##\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent/L-BFGS\n",
    "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2     # larger values converge faster to less accurate results\n",
    "TARGETED = True          # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0.5           # how strong the adversarial example should be\n",
    "INITIAL_CONST = 0.05     # the initial constant c to pick as a first guess, Manages the contiribution of the loss due to flow or spatial regularization loss.\n",
    "\n",
    "class STAdv:\n",
    "    def __init__(self, sess, model, batch_size=1, confidence = CONFIDENCE,\n",
    "                 targeted = TARGETED, learning_rate = LEARNING_RATE,\n",
    "                 binary_search_steps = BINARY_SEARCH_STEPS, max_iterations = MAX_ITERATIONS,\n",
    "                 abort_early = ABORT_EARLY, \n",
    "                 initial_const = INITIAL_CONST,\n",
    "                 boxmin = -0.5, boxmax = 0.5):\n",
    "        \"\"\"\n",
    "        The Spatial transformed adversarial attack. \n",
    "        This attack is based on Spatially transformed adversarial examples paper.\n",
    "        Returns adversarial examples for the supplied model.\n",
    "        \n",
    "        confidence: Confidence of adversarial examples: higher produces examples\n",
    "              that are farther away, but more strongly classified as adversarial.\n",
    "        batch_size: Number of attacks to run simultaneously.\n",
    "        targeted: True if we should perform a targetted attack, False otherwise.\n",
    "        learning_rate: The learning rate for the attack algorithm. Smaller values\n",
    "              produce better results but are slower to converge.\n",
    "        binary_search_steps: The number of times we perform binary search to\n",
    "              find the optimal tradeoff-constant between distance and confidence. \n",
    "        max_iterations: The maximum number of iterations. Larger values are more\n",
    "              accurate; setting too small will require a large learning rate and will\n",
    "              produce poor results.\n",
    "        abort_early: If true, allows early aborts if gradient descent gets stuck.\n",
    "        initial_const: The initial tradeoff-constant to use to tune the relative\n",
    "              importance of distance and confidence. If binary_search_steps is large,\n",
    "              the initial constant is not important.\n",
    "        \"\"\"\n",
    "\n",
    "        image_size, num_channels, num_labels = model.image_size, model.num_channels, model.num_labels\n",
    "        self.sess = sess\n",
    "        self.TARGETED = targeted\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.BINARY_SEARCH_STEPS = binary_search_steps\n",
    "        self.ABORT_EARLY = abort_early\n",
    "        self.CONFIDENCE = confidence\n",
    "        self.initial_const = initial_const\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.repeat = binary_search_steps >= 10\n",
    "\n",
    "        shape = (batch_size,image_size,image_size,num_channels)\n",
    "        \n",
    "        # the variable we're going to optimize over\n",
    "        modifier = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "        # these are variables to be more efficient in sending data to tf\n",
    "        self.timg = tf.Variable(np.zeros(shape), dtype=tf.float32)\n",
    "        self.tlab = tf.Variable(np.zeros((batch_size,num_labels)), dtype=tf.float32)\n",
    "        self.const = tf.Variable(np.zeros(batch_size), dtype=tf.float32)\n",
    "\n",
    "        # and here's what we use to assign them\n",
    "        self.assign_timg = tf.placeholder(tf.float32, shape)\n",
    "        self.assign_tlab = tf.placeholder(tf.float32, (batch_size,num_labels))\n",
    "        self.assign_const = tf.placeholder(tf.float32, [batch_size])\n",
    "        \n",
    "        # the resulting image, tanh'd to keep bounded from boxmin to boxmax\n",
    "#         self.boxmul = (boxmax - boxmin) / 2.\n",
    "#         self.boxplus = (boxmin + boxmax) / 2.\n",
    "#         self.newimg = tf.tanh(modifier + self.timg) * self.boxmul + self.boxplus\n",
    "        self.newimg = modifier + self.timg\n",
    "        \n",
    "        # Create the grid for bilinear interpolation\n",
    "        indices_grid = self._meshgrid(image_size, image_size)\n",
    "        print(\"meshgrid shape\", indices_grid.shape)\n",
    "        indices_grid = tf.tile(indices_grid, tf.stack([batch_size]))\n",
    "        print(\"batch_tile grid shape\", indices_grid.shape)\n",
    "        indices_grid = tf.reshape(indices_grid, (batch_size, 3, -1))\n",
    "        print(\"batch+3:\", indices_grid.shape)\n",
    "        transformed_grid = indices_grid\n",
    "        x_s = tf.slice(transformed_grid, [0, 0, 0], [-1, 1, -1])\n",
    "        print(\"grid x_s shape:\", x_s.shape)\n",
    "        y_s = tf.slice(transformed_grid, [0, 1, 0], [-1, 1, -1])\n",
    "\n",
    "        x_s_flatten = tf.reshape(x_s, [-1])\n",
    "        y_s_flatten = tf.reshape(y_s, [-1])\n",
    "        print(\"x_flatten_shape:\", x_s_flatten.shape)\n",
    "        # TODO: apply biliear interpolation after adding the modifer?\n",
    "        # bilinear interpolation\n",
    "        output_size = (image_size, image_size)\n",
    "        self.newimg = self._interpolate(self.newimg,\n",
    "                                 x_s_flatten,\n",
    "                                 y_s_flatten,\n",
    "                                 output_size)\n",
    "        self.newimg = tf.reshape(self.newimg, shape)\n",
    "        \n",
    "        self.newimg = tf.cast(self.newimg, dtype='float32')\n",
    "        print(self.newimg.shape)\n",
    "        \n",
    "        # prediction BEFORE-SOFTMAX of the model\n",
    "        self.output = model.predict(self.newimg)\n",
    "        \n",
    "        # distance to the input data\n",
    "        ## TODO : figure out the exact total variation loss used in the paper\n",
    "        # the following version works too\n",
    "        self.stdist = self.total_variation_based_loss(self.newimg- self.timg)\n",
    "#         self.stdist = self.total_variation_based_loss(self.newimg)\n",
    "        self.stdist = tf.cast(self.stdist,  dtype='float32')\n",
    "    \n",
    "        # compute the probability of the label class versus the maximum other\n",
    "        real = tf.reduce_sum((self.tlab)*self.output, 1)\n",
    "        other = tf.reduce_max((1.-self.tlab)*self.output - (self.tlab*10000.), 1)\n",
    "        \n",
    "        #TODO: improve on the value of loss1 as its 0 most of the time.\n",
    "        if self.TARGETED:\n",
    "            # if targetted, optimize for making the other class most likely\n",
    "            loss1 = tf.maximum(0.0, other-real+self.CONFIDENCE)\n",
    "        else:\n",
    "            # if untargeted, optimize for making this class least likely.\n",
    "            loss1 = tf.maximum(0.0, real-other+self.CONFIDENCE)\n",
    "        \n",
    "        # sum up the losses\n",
    "        self.loss2 = tf.reduce_sum(self.const*self.stdist)\n",
    "        self.loss1 = tf.reduce_sum(loss1)\n",
    "        self.loss = self.loss1 + self.loss2\n",
    "        \n",
    "        # Setup the adam optimizer and keep track of variables we're creating\n",
    "        start_vars = set(x.name for x in tf.global_variables())\n",
    "        self.optimizer = optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss,\n",
    "                method='L-BFGS-B',  var_list = [modifier], var_to_bounds={modifier:([0,1])}, # TODO: bound the new image not just the modifier\n",
    "                options={'maxiter': max_iterations}, \n",
    "                )\n",
    "#         optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n",
    "#         self.train = optimizer.minimize(self.loss, var_list=[modifier])\n",
    "        end_vars = tf.global_variables()\n",
    "        new_vars = [x for x in end_vars if x.name not in start_vars]\n",
    "\n",
    "        # these are the variables to initialize when we run\n",
    "        self.setup = []\n",
    "        self.setup.append(self.timg.assign(self.assign_timg))\n",
    "        self.setup.append(self.tlab.assign(self.assign_tlab))\n",
    "        self.setup.append(self.const.assign(self.assign_const))\n",
    "        \n",
    "        self.init = tf.variables_initializer(var_list=[modifier]+new_vars)\n",
    "    \n",
    "    def total_variation_based_loss(self, images):\n",
    "        \"\"\"\n",
    "          Calculate and return the total variation based loss for one or more images.\n",
    "          The total variation is the sum of the L2 norm for neighboring\n",
    "          pixel-values in the input images. This measures how much noise is in the\n",
    "          images.\n",
    "          This is based on the total variation of images in the tensorflow with the \n",
    "          following documention:\n",
    "          Total Variation:\n",
    "          This can be used as a loss-function during optimization so as to suppress\n",
    "          noise in images. If you have a batch of images, then you should calculate\n",
    "          the scalar loss-value as the sum:\n",
    "          `loss = tf.reduce_sum(tf.image.total_variation(images))`\n",
    "          This implements the anisotropic 2-D version of the formula described here:\n",
    "          https://en.wikipedia.org/wiki/Total_variation_denoising\n",
    "          Args:\n",
    "            images: 4-D Tensor of shape `[batch, height, width, channels]` or\n",
    "                    3-D Tensor of shape `[height, width, channels]`.\n",
    "            name: A name for the operation (optional).\n",
    "          Raises:\n",
    "            ValueError: if images.shape is not a 3-D or 4-D vector.\n",
    "          Returns:\n",
    "            The total variation of `images`.\n",
    "            If `images` was 4-D, return a 1-D float Tensor of shape `[batch]` with the\n",
    "            total variation for each image in the batch.\n",
    "            If `images` was 3-D, return a scalar float with the total variation for\n",
    "            that image.\n",
    "          \"\"\"\n",
    "        ndims = images.get_shape().ndims\n",
    "\n",
    "        if ndims == 3:\n",
    "            # The input is a single image with shape [height, width, channels].\n",
    "            # Calculate the difference of neighboring pixel-values.\n",
    "            # The images are shifted one pixel along the height and width by slicing.\n",
    "            pixel_dif1 = images[1:, :, :] - images[:-1, :, :]\n",
    "            pixel_dif2 = images[:, 1:, :] - images[:, :-1, :]\n",
    "            \n",
    "            # Sum for all axis. (None is an alias for all axis.)\n",
    "            sum_axis = None\n",
    "        elif ndims == 4:\n",
    "            # The input is a batch of images with shape:\n",
    "            # [batch, height, width, channels].\n",
    "            \n",
    "            # Calculate the difference of neighboring pixel-values.\n",
    "            # The images are shifted one pixel along the height and width by slicing.\n",
    "            pixel_dif1 = images[:, 1:, :, :] - images[:, :-1, :, :]\n",
    "            pixel_dif2 = images[:, :, 1:, :] - images[:, :, :-1, :]\n",
    "\n",
    "            # Only sum for the last 3 axis.\n",
    "            # This results in a 1-D tensor with the total variation for each image.\n",
    "            sum_axis = [1, 2, 3]\n",
    "        else:\n",
    "            raise ValueError('\\'images\\' must be either 3 or 4-dimensional.')\n",
    "\n",
    "        # Calculate the total variation by taking the absolute value of the\n",
    "        # pixel-differences and summing over the appropriate axis.\n",
    "        tot_var = (\n",
    "            tf.add(\n",
    "#                 tf.nn.l2_loss(pixel_dif1**2),\n",
    "#                 tf.nn.l2_loss(pixel_dif2**2)))\n",
    "            tf.reduce_sum(tf.square(pixel_dif1), axis=sum_axis) ,\n",
    "            tf.reduce_sum(tf.square(pixel_dif2), axis=sum_axis)\n",
    "            ))\n",
    "        return tot_var\n",
    "\n",
    "\n",
    "            # grid sampling, returns a tensor of size hxwx3\n",
    "    def _meshgrid(self, height, width):\n",
    "        x_linspace = tf.linspace(-1., 1., width)\n",
    "        y_linspace = tf.linspace(-1., 1., height)\n",
    "        x_coordinates, y_coordinates = tf.meshgrid(x_linspace, y_linspace)\n",
    "        x_coordinates = tf.reshape(x_coordinates, [-1])\n",
    "        y_coordinates = tf.reshape(y_coordinates, [-1])\n",
    "        ones = tf.ones_like(x_coordinates)\n",
    "        indices_grid = tf.concat([x_coordinates, y_coordinates, ones], 0)\n",
    "        return indices_grid\n",
    "\n",
    "    def _repeat(self, x, num_repeats):\n",
    "            ones = tf.ones((1, num_repeats), dtype='int32')\n",
    "            x = tf.reshape(x, shape=(-1,1))\n",
    "            x = tf.matmul(x, ones)\n",
    "            return tf.reshape(x, [-1])\n",
    "\n",
    "    # bilinear interpolation\n",
    "    def _interpolate(self, image, x, y, output_size):\n",
    "        batch_size = tf.shape(image)[0]\n",
    "        height = tf.shape(image)[1]\n",
    "        width = tf.shape(image)[2]\n",
    "        num_channels = tf.shape(image)[3]\n",
    "\n",
    "        x = tf.cast(x , dtype='float32')\n",
    "        y = tf.cast(y , dtype='float32')\n",
    "\n",
    "        height_float = tf.cast(height, dtype='float32')\n",
    "        width_float = tf.cast(width, dtype='float32')\n",
    "\n",
    "        output_height = output_size[0]\n",
    "        output_width  = output_size[1]\n",
    "        \n",
    "        # because the intial values are from -1 and 1, scale them to the original coordinates\n",
    "        x = .5*(x + 1.0)*(width_float)\n",
    "        y = .5*(y + 1.0)*(height_float)\n",
    "        \n",
    "        # Get the lower x coordinate\n",
    "        x0 = tf.cast(tf.floor(x), 'int32')\n",
    "        # upper x coordinate is just one plus the lower x\n",
    "        x1 = x0 + 1\n",
    "        y0 = tf.cast(tf.floor(y), 'int32')\n",
    "        y1 = y0 + 1\n",
    "        \n",
    "        # max y and max x for clipping the coordinates\n",
    "        max_y = tf.cast(height - 1, dtype='int32')\n",
    "        max_x = tf.cast(width - 1,  dtype='int32')\n",
    "        zero = tf.zeros([], dtype='int32')\n",
    "    \n",
    "        # Clip the coordinates\n",
    "        x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "        x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "        y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "        y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "        \n",
    "        # flatten the pixel values of input img\n",
    "        flat_image_dimensions = width*height\n",
    "        pixels_batch = tf.range(batch_size)*flat_image_dimensions\n",
    "        # flatten the output img dimensions\n",
    "        flat_output_dimensions = output_height*output_width\n",
    "        # get a array of ones for the output flattened dim\n",
    "        base = self._repeat(pixels_batch, flat_output_dimensions)\n",
    "        \n",
    "        print(\"base shape:\", base.shape)\n",
    "        base_y0 = base + y0*width\n",
    "        print(\"base_y0_shape\", base_y0.shape)\n",
    "        base_y1 = base + y1*width\n",
    "        indices_a = base_y0 + x0\n",
    "        print(\"indices_shape\", indices_a.shape)\n",
    "        indices_b = base_y1 + x0\n",
    "        indices_c = base_y0 + x1\n",
    "        indices_d = base_y1 + x1\n",
    "\n",
    "        flat_image = tf.reshape(image, shape=(-1, num_channels))\n",
    "        flat_image = tf.cast(flat_image, dtype='float32')\n",
    "        # get the pixel values corresponding to the indices from the image.\n",
    "        pixel_values_a = tf.gather(flat_image, indices_a)\n",
    "        pixel_values_b = tf.gather(flat_image, indices_b)\n",
    "        pixel_values_c = tf.gather(flat_image, indices_c)\n",
    "        pixel_values_d = tf.gather(flat_image, indices_d)\n",
    "        \n",
    "        # cast to float\n",
    "        x0 = tf.cast(x0, 'float32')\n",
    "        x1 = tf.cast(x1, 'float32')\n",
    "        y0 = tf.cast(y0, 'float32')\n",
    "        y1 = tf.cast(y1, 'float32')\n",
    "        \n",
    "        # Get the 4 areas calculated for the 4 coordinate points and the input point.\n",
    "        area_a = tf.expand_dims(((x1 - x) * (y1 - y)), 1)\n",
    "        area_b = tf.expand_dims(((x1 - x) * (y - y0)), 1)\n",
    "        area_c = tf.expand_dims(((x - x0) * (y1 - y)), 1)\n",
    "        area_d = tf.expand_dims(((x - x0) * (y - y0)), 1)\n",
    "        output = tf.add_n([area_a*pixel_values_a,\n",
    "                           area_b*pixel_values_b,\n",
    "                           area_c*pixel_values_c,\n",
    "                           area_d*pixel_values_d])\n",
    "        return output\n",
    "\n",
    "    def attack(self, imgs, targets):\n",
    "        \"\"\"\n",
    "        Perform the stAdv attack on the given images for the given targets.\n",
    "        If self.targeted is true, then the targets represents the target labels.\n",
    "        If self.targeted is false, then targets are the original class labels.\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        print('go up to', len(imgs))\n",
    "        for i in range(0,len(imgs),self.batch_size):\n",
    "            print('attack for image', i)\n",
    "            r.extend(self.attack_batch(imgs[i:i+self.batch_size], targets[i:i+self.batch_size]))\n",
    "        return np.array(r)\n",
    "\n",
    "    def attack_batch(self, imgs, labs):\n",
    "        \"\"\"\n",
    "        Run the attack on a batch of images and labels.\n",
    "        \"\"\"\n",
    "        def compare(x,y):\n",
    "            if not isinstance(x, (float, int, np.int64)):\n",
    "                x = np.copy(x)\n",
    "                if self.TARGETED:\n",
    "                    x[y] -= self.CONFIDENCE\n",
    "                else:\n",
    "                    x[y] += self.CONFIDENCE\n",
    "                x = np.argmax(x)\n",
    "            if self.TARGETED:\n",
    "                return x == y\n",
    "            else:\n",
    "                return x != y\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # convert to tanh-space\n",
    "#         imgs = np.arctanh((imgs - self.boxplus) / self.boxmul * 0.999999)\n",
    "\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        CONST = np.ones(batch_size)*self.initial_const\n",
    "        upper_bound = np.ones(batch_size)*1e10\n",
    "\n",
    "        # the best stAdv, score, and image attack\n",
    "        o_best_stAdv = [1e10]*batch_size\n",
    "        o_bestscore = [-1]*batch_size\n",
    "        # best image for the attack\n",
    "        o_bestattack = [np.zeros(imgs[0].shape)]*batch_size\n",
    "        \n",
    "        # binary seach for the correct c and also the best example.\n",
    "        for outer_step in range(self.BINARY_SEARCH_STEPS):\n",
    "            print(\"outer step:\", outer_step)\n",
    "#             print(\"o_best_attack_img:\", o_bestattack)\n",
    "            \n",
    "            # completely reset adam's internal state.\n",
    "            self.sess.run(self.init)\n",
    "            batch = imgs[:batch_size]\n",
    "            batchlab = labs[:batch_size]\n",
    "            beststAdv = [1e10]*batch_size\n",
    "            bestscore = [-1]*batch_size\n",
    "\n",
    "            # The last iteration (if we run many steps) repeat the search once.\n",
    "            if self.repeat == True and outer_step == self.BINARY_SEARCH_STEPS-1:\n",
    "                CONST = upper_bound\n",
    "\n",
    "            # set the variables so that we don't have to send them over again\n",
    "            self.sess.run(self.setup, {self.assign_timg: batch,\n",
    "                                       self.assign_tlab: batchlab,\n",
    "                                       self.assign_const: CONST})\n",
    "            \n",
    "            prev = 1e6\n",
    "            \n",
    "            for iteration in range(self.MAX_ITERATIONS):\n",
    "                # perform the attack \n",
    "                self.optimizer.minimize(self.sess)\n",
    "                l, stAdvs, scores, nimg = self.sess.run([\n",
    "#                                                        self.train,\n",
    "                                                         self.loss, \n",
    "                                                         self.stdist,\n",
    "                                                         self.output, \n",
    "                                                         self.newimg])\n",
    "\n",
    "                # print out the losses every 10%\n",
    "                if iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    print(\"iter:\", iteration, \"total_loss, L_adv, L_flow:\", self.sess.run((self.loss,self.loss1,self.loss2)))\n",
    "\n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if self.ABORT_EARLY and iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    if l > prev*.9999:\n",
    "                        break\n",
    "                    prev = l\n",
    "\n",
    "                # adjust the best result found so far\n",
    "                for e,(stAdv,sc,ii) in enumerate(zip(stAdvs,scores,nimg)):\n",
    "                    if stAdv < beststAdv[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        beststAdv[e] = stAdv\n",
    "                        bestscore[e] = np.argmax(sc)\n",
    "                    if stAdv < o_best_stAdv[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        o_best_stAdv[e] = stAdv\n",
    "                        o_bestscore[e] = np.argmax(sc)\n",
    "                        o_bestattack[e] = ii\n",
    "\n",
    "            # adjust the constant as needed through binary search\n",
    "            for e in range(batch_size):\n",
    "                if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e] != -1:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    #          or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "\n",
    "        # return the best solution found\n",
    "        o_best_stAdv = np.array(o_best_stAdv)\n",
    "        return o_bestattack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run STAdv attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## test_attack.py -- sample code to test attack procedure\n",
    "##\n",
    "## Copyright (C) 2016, Nicholas Carlini <nicholas@carlini.com>.\n",
    "##\n",
    "## This program is licenced under the BSD 2-Clause licence,\n",
    "## contained in the LICENCE file in this directory.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(img):\n",
    "    \"\"\"\n",
    "    Show MNSIT digits in the console.\n",
    "    \"\"\"\n",
    "    remap = \"  .*#\"+\"#\"*100\n",
    "    img = (img.flatten()+.5)*3\n",
    "    if len(img) != 784: return\n",
    "    print(\"START\")\n",
    "    for i in range(28):\n",
    "        print(\"\".join([remap[int(round(x))] for x in img[i*28:i*28+28]]))\n",
    "\n",
    "def plot(img, image_size, title):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(img.reshape(image_size, image_size), cmap='gray', interpolation='none')\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def generate_data(data, samples, targeted=True, start=0, inception=False):\n",
    "    \"\"\"\n",
    "    Generate the input data to the attack algorithm.\n",
    "    data: the images to attack\n",
    "    samples: number of samples to use\n",
    "    targeted: if true, construct targeted attacks, otherwise untargeted attacks\n",
    "    start: offset into data to use\n",
    "    inception: if targeted and inception, randomly sample 100 targets intead of 1000\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    # generating data from test_data inputs and then the targets\n",
    "    for i in range(samples):\n",
    "        if targeted:\n",
    "            if inception:\n",
    "                seq = random.sample(range(1,1001), 10)\n",
    "            else:\n",
    "                seq = range(data.test_labels.shape[1])\n",
    "\n",
    "            for j in seq:\n",
    "                if (j == np.argmax(data.test_labels[start+i])) and (inception == False):\n",
    "                    continue\n",
    "                inputs.append(data.test_data[start+i])\n",
    "                targets.append(np.eye(data.test_labels.shape[1])[j])\n",
    "        else:\n",
    "            inputs.append(data.test_data[start+i])\n",
    "            targets.append(data.test_labels[start+i])\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10.244497, 1.4305115e-06, 10.244495))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (10.24416, 0.0, 10.24416))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.324176, 0.0, 9.324176))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.324174, 0.0, 9.324174))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.351032, 5.9127808e-05, 5.3509727))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.34548, 0.0, 5.34548))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (5.34548, 0.0, 5.34548))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.3498957, 0.0, 3.3498957))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.3498957, 0.0, 3.3498957))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.2804272, 0.0, 2.2804272))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.2804272, 0.0, 2.2804272))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.4715762, 0.0, 1.4715762))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.4715762, 0.0, 1.4715762))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.94997734, 0.0, 0.94997734))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.8310955, 0.0, 0.8310955))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.8310955, 0.0, 0.8310955))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.6537576, 0.0, 0.6537576))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.5897341, 0.0, 0.5897341))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.5897341, 0.0, 0.5897341))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.5370371, 0.0, 0.5370371))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.45190465, 0.0, 0.45190465))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.45190465, 0.0, 0.45190465))\n",
      "('Took', 163.89953112602234, 'seconds to run', 9, 'samples.')\n",
      "('adv shape:', (9, 28, 28, 1))\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import random \n",
    "start = random.randint(0,10000)\n",
    "with tf.Session() as sess:\n",
    "    data, model =  MNIST(), MNISTModel(\"models/mnist_baseline\", sess)\n",
    "    attack = STAdv(sess, model, batch_size=9, max_iterations=1000, confidence=0)\n",
    "    \n",
    "    inputs, targets = generate_data(data, samples=1, targeted=True,\n",
    "                                    start=start, inception=False)\n",
    "    timestart = time.time()\n",
    "    adv = attack.attack(inputs, targets)\n",
    "    timeend = time.time()\n",
    "\n",
    "    print(\"Took\",timeend-timestart,\"seconds to run\",len(inputs),\"samples.\")\n",
    "    \n",
    "    image_size = 28\n",
    "    print(\"adv shape:\", adv.shape)\n",
    "    np.save(\"adv_examples\", adv)\n",
    "    for i in range(len(adv)):\n",
    "        print(\"adv[i]\", adv[i])\n",
    "        print(\"Valid:\")\n",
    "        show(inputs[i])\n",
    "#         plot(inputs[i], image_size, \"valid\")\n",
    "        print(\"Adversarial:\")\n",
    "        show(adv[i])\n",
    "        \n",
    "#         plot(inputs[i],image_size, \"adv\")\n",
    "        prediction = model.model.predict(adv[i:i+1])\n",
    "        print(\"Classification:\", prediction)\n",
    "\n",
    "        print(\"classification:\", np.argmax(prediction))\n",
    "        num_pixels = model.image_size**2\n",
    "        print(\"Total distortion:\", sess.run(STAdv.total_variation_based_loss(attack, tf.convert_to_tensor(adv[i]-inputs[i]))) **.5/num_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Spatially adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.395704, 0.0006556511, 9.395048))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.394525, 0.0, 9.394525))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (9.394525, 0.0, 9.394525))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (11.16351, 0.0, 11.16351))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.534014, 0.0, 5.534014))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (5.534014, 0.0, 5.534014))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.9993043, 0.0, 4.9993043))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.9993043, 0.0, 4.9993043))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.755704, 0.0, 3.755704))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.755704, 0.0, 3.755704))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.8711528, 0.0, 1.8711528))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.8711528, 0.0, 1.8711528))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.9718829, 0.0, 0.9718829))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.8256425, 0.0, 0.8256425))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.8256425, 0.0, 0.8256425))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.48630303, 0.0, 0.48630303))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.48630303, 0.0, 0.48630303))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.28111032, 0.0, 0.28111032))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.28111026, 0.0, 0.28111026))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.18016359, 0.0, 0.18016359))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.18016319, 0.0, 0.18016319))\n",
      "('Took', 151.10582780838013, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.521823, 0.0, 9.521823))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.521813, 0.0, 9.521813))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.9586806, 0.0, 5.9586806))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.9586806, 0.0, 5.9586806))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.4347124, 0.0, 4.4347124))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.4347124, 0.0, 4.4347124))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.2678113, 0.0, 3.2678113))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.2678113, 0.0, 3.2678113))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.8967273, 0.0, 1.8967273))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.8967273, 0.0, 1.8967273))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.1242288, 0.0, 1.1242288))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.1242288, 0.0, 1.1242288))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.5345745, 0.0, 0.5345745))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.5345745, 0.0, 0.5345745))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.28101498, 0.0, 0.28101498))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.28101498, 0.0, 0.28101498))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.14032693, 0.0, 0.14032693))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.14032693, 0.0, 0.14032693))\n",
      "('Took', 117.14717507362366, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.811909, 0.0, 9.811909))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.811909, 0.0, 9.811909))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.599584, 0.0, 4.599584))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.599584, 0.0, 4.599584))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.8587847, 0.0, 3.8587847))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.8587837, 0.0, 3.8587837))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.1360404, 0.0, 3.1360404))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.0294685, 0.0, 3.0294685))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (3.0294685, 0.0, 3.0294685))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.753696, 0.0, 1.753696))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.753696, 0.0, 1.753696))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.5399144, 0.0, 1.5399144))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.1390687, 0.0, 1.1390687))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.1390687, 0.0, 1.1390687))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.5119826, 0.0, 1.5119826))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.5380152, 0.0, 0.5380152))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.5380152, 0.0, 0.5380152))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.7140019, 0.0, 0.7140019))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.71400183, 0.0, 0.71400183))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.8640335, 0.0, 0.8640335))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.84560174, 0.0, 0.84560174))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.84560174, 0.0, 0.84560174))\n",
      "('Took', 159.970862865448, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10.825604, 0.06463909, 10.760965))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (10.619755, 0.0, 10.619755))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (10.619755, 0.0, 10.619755))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.882196, 0.01121676, 5.8709793))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.854778, 0.0, 5.854778))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (5.854778, 0.0, 5.854778))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.9712768, 0.1366396, 4.834637))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.7023892, 0.0, 4.7023892))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (4.7023892, 0.0, 4.7023892))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.4657698, 0.0, 3.4657698))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.4657693, 0.0, 3.4657693))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.188377, 0.0, 2.188377))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.188376, 0.0, 2.188376))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.5046722, 0.0065664053, 1.4981058))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.3075193, 0.0, 1.3075193))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.3075193, 0.0, 1.3075193))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.8123337, 0.0, 0.8123337))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.8123337, 0.0, 0.8123337))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.58164805, 0.0, 0.58164805))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.58107346, 0.0, 0.58107346))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.58107346, 0.0, 0.58107346))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.5916544, 0.0, 0.5916544))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.47172755, 0.0, 0.47172755))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.47172755, 0.0, 0.47172755))\n",
      "('Took', 186.81285905838013, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.475181, 0.0, 9.475181))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.475181, 0.0, 9.475181))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (14.610016, 0.26551294, 14.344503))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.490545, 4.7683716e-07, 9.490545))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (9.490545, 4.7683716e-07, 9.490545))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (32.879814, 3.07727, 29.802544))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (32.64999, 3.1158726, 29.534119))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (32.64999, 3.1158726, 29.534119))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (169.01154, 33.323433, 135.6881))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (168.7685, 33.45317, 135.31532))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (168.7685, 33.45317, 135.31532))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1340.1628, 38.742542, 1301.4203))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1340.1626, 38.74269, 1301.4199))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (13036.69, 39.248947, 12997.441))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (13036.69, 39.248947, 12997.441))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (130010.56, 43.611137, 129966.95))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (130010.56, 43.611137, 129966.95))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1299730.5, 63.883488, 1299666.6))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (12996730.0, 63.91333, 12996666.0))\n",
      "('Took', 127.72926712036133, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.869596, 0.0, 9.869596))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.869596, 0.0, 9.869596))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (13.639436, 0.0, 13.639436))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (13.606248, 0.0, 13.606248))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (13.606248, 0.0, 13.606248))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (7.732398, 0.0, 7.732398))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (7.732398, 0.0, 7.732398))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.0943995, 0.0, 5.0943995))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.0943995, 0.0, 5.0943995))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.2053978, 0.0, 3.2053978))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.2053974, 0.0, 3.2053974))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.9414907, 0.0, 2.9414907))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.9414907, 0.0, 2.9414907))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.7725365, 0.0, 1.7725365))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.7725365, 0.0, 1.7725365))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.057195, 0.0, 2.057195))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.057195, 0.0, 2.057195))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.7325035, 0.0, 1.7325035))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.2896228, 0.0, 1.2896228))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.2896228, 0.0, 1.2896228))\n",
      "('Took', 139.39468097686768, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (8.4289665, 0.0, 8.4289665))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (8.4289665, 0.0, 8.4289665))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (6.8761525, 0.013980269, 6.862172))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (6.4071436, 0.0, 6.4071436))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (6.4071436, 0.0, 6.4071436))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.310807, 0.0, 4.310807))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.3108044, 0.0, 4.3108044))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.4483175, 0.014580369, 3.433737))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.60925, 0.0, 2.60925))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.60925, 0.0, 2.60925))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.9407599, 0.0, 1.9407599))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.9407591, 0.0, 1.9407591))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.1024215, 0.0, 1.1024215))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.77700907, 0.0, 0.77700907))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (0.77700907, 0.0, 0.77700907))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.40146422, 0.0, 0.40146422))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.40146422, 0.0, 0.40146422))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.26508334, 0.0, 0.26508334))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.26508334, 0.0, 0.26508334))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.12189323, 0.0, 0.12189323))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.12189323, 0.0, 0.12189323))\n",
      "('Took', 152.91827297210693, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10.06089, 0.019666195, 10.041224))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (7.601181, 0.0, 7.601181))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (7.601181, 0.0, 7.601181))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (6.504893, 0.011694431, 6.4931984))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (6.49095, 0.0, 6.49095))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (6.49095, 0.0, 6.49095))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.8325338, 0.0, 2.8325338))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.8325307, 0.0, 2.8325307))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.9099374, 0.0040295124, 1.9059079))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.9045491, 0.0, 1.9045491))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.9045491, 0.0, 1.9045491))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.93267214, 0.0, 0.93267214))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.93267214, 0.0, 0.93267214))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.7670934, 0.0, 0.7670934))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.7670932, 0.0, 0.7670932))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.37202588, 0.0, 0.37202588))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.37202588, 0.0, 0.37202588))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.15873292, 0.0, 0.15873292))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.15873292, 0.0, 0.15873292))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.09955147, 0.0, 0.09955147))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.09955134, 0.0, 0.09955134))\n",
      "('Took', 152.18768000602722, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (8.356224, 0.0, 8.356224))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (8.356224, 0.0, 8.356224))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.4349933, 0.005689621, 5.4293036))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.0077574, 0.0, 3.0077574))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (3.0077574, 0.0, 3.0077574))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.2571554, 0.07336235, 4.183793))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.3173218, 0.0, 3.3173218))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (3.3173218, 0.0, 3.3173218))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.7607453, 0.0, 2.7607453))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.7607436, 0.0, 2.7607436))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.3687792, 0.0, 1.3687792))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.368779, 0.0, 1.368779))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.6129634, 0.0, 0.6129634))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.61296326, 0.0, 0.61296326))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.343173, 0.0, 0.343173))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.343173, 0.0, 0.343173))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.16182396, 0.0, 0.16182396))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.16182396, 0.0, 0.16182396))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.085130796, 0.0, 0.085130796))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.085130796, 0.0, 0.085130796))\n",
      "('Took', 136.4108169078827, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10.033891, 0.00879097, 10.0251))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (10.026987, 0.0, 10.026987))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (10.026987, 0.0, 10.026987))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (13.393189, 0.0, 13.393189))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (6.8794866, 0.0, 6.8794866))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (6.8794866, 0.0, 6.8794866))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.023728, 0.014761448, 5.0089664))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.0093546, 0.0, 5.0093546))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (5.0093546, 0.0, 5.0093546))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.057235, 0.0, 3.057235))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.057235, 0.0, 3.057235))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.6467334, 0.016884804, 1.6298486))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.5169044, 0.0, 1.5169044))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.5169044, 0.0, 1.5169044))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.9520856, 0.0, 0.9520856))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.9520856, 0.0, 0.9520856))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.51929945, 0.0, 0.51929945))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.51929945, 0.0, 0.51929945))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.2618145, 0.0, 0.2618145))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.2618145, 0.0, 0.2618145))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (0.1318973, 0.0, 0.1318973))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (0.1318973, 0.0, 0.1318973))\n",
      "('Took', 161.84403800964355, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (12.362268, 0.0, 12.362268))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (12.3622675, 0.0, 12.3622675))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (12.613357, 0.0038290024, 12.609528))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (11.268711, 0.0, 11.268711))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (11.268711, 0.0, 11.268711))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (7.4392233, 0.00044584274, 7.4387774))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (7.1480966, 0.0, 7.1480966))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (7.1480966, 0.0, 7.1480966))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10.91305, 0.0, 10.91305))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (10.91305, 0.0, 10.91305))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (6.995515, 0.0, 6.995515))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (6.995515, 0.0, 6.995515))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.961473, 0.007036209, 4.954437))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.111161, 0.0, 4.111161))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (4.111161, 0.0, 4.111161))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.2628083, 0.0, 4.2628083))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4.2628083, 0.0, 4.2628083))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.9167225, 0.048101425, 3.868621))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.8671038, 0.0, 3.8671038))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (3.8671038, 0.0, 3.8671038))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (7.274888, 0.011065006, 7.263823))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.614921, 0.0, 3.614921))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (3.614921, 0.0, 3.614921))\n",
      "('Took', 178.18965196609497, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.3712456, 0.13444546, 2.2368002))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.3710532, 0.13440186, 2.2366514))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5.544974, 0.40870386, 5.13627))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5.544974, 0.40870386, 5.13627))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (43.53209, 0.46244055, 43.06965))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (43.53209, 0.46244055, 43.06965))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (426.99384, 0.47147244, 426.52237))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (426.9312, 0.47147724, 426.45975))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (426.9312, 0.47147724, 426.45975))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4265.101, 1.4867724, 4263.6143))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4264.1934, 0.8741805, 4263.3193))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (4264.1934, 0.8741805, 4263.3193))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (42625.48, 1.5883467, 42623.89))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (42623.266, 1.5858687, 42621.68))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (426208.78, 1.6019881, 426207.2))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (426208.78, 1.6019881, 426207.2))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4262064.5, 1.6281394, 4262063.0))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (42622790.0, 1.7548065, 42622790.0))\n",
      "('Took', 125.46442818641663, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.6054658, 0.0001578778, 1.6053079))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.5495268, 2.2217631e-05, 1.5495046))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.5495268, 2.2217631e-05, 1.5495046))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.2487202, 0.26322508, 2.985495))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.2484071, 0.26313752, 2.9852695))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (24.236267, 0.33617854, 23.90009))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (24.235865, 0.33621815, 23.899647))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (236.3634, 0.34846267, 236.01494))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (236.3634, 0.34846267, 236.01494))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2359.6907, 0.55636334, 2359.1343))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2359.6907, 0.55636334, 2359.1343))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (23584.38, 2.0288954, 23582.352))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (23584.38, 2.0288954, 23582.352))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (235818.48, 2.0335178, 235816.45))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (235818.48, 2.0335178, 235816.45))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2358159.8, 2.0445585, 2358157.8))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (23581542.0, 2.1734467, 23581540.0))\n",
      "('Took', 130.42090010643005, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.0480077, 0.1474834, 1.9005244))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.0396397, 0.14741477, 1.8922249))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.0396397, 0.14741477, 1.8922249))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (8.298838, 0.88010246, 7.4187355))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (8.298615, 0.8798889, 7.418727))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (71.54126, 1.0268261, 70.514435))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (71.53194, 1.0264537, 70.505486))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (71.53194, 1.0264537, 70.505486))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (704.4206, 1.0478, 703.3728))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (704.4203, 1.0478113, 703.3725))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (7035.096, 1.3522911, 7033.744))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (7034.272, 1.2032691, 7033.069))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (7034.272, 1.2032691, 7033.069))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (70326.62, 1.4504397, 70325.164))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (70326.62, 1.4504397, 70325.164))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (703247.75, 1.5078685, 703246.25))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (703247.75, 1.5078685, 703246.25))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (7032453.0, 1.5213213, 7032451.5))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (70324540.0, 1.5384262, 70324540.0))\n",
      "('Took', 147.17590808868408, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.6103557, 0.092079096, 1.5182767))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.6102886, 0.09204797, 1.5182407))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.0252976, 0.31327027, 1.7120274))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.0159094, 0.3131171, 1.7027924))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.0159094, 0.3131171, 1.7027924))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (11.152348, 0.34907627, 10.803271))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (11.152345, 0.34907743, 10.8032675))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (103.85083, 0.3522164, 103.49861))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (103.85002, 0.35217395, 103.49785))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1034.4008, 1.060817, 1033.34))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1033.1376, 0.4426448, 1032.695))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1033.1376, 0.4426448, 1032.695))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10328.037, 1.6050911, 10326.432))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (10319.845, 1.5086869, 10318.336))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (10319.845, 1.5086869, 10318.336))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (103191.95, 1.6135107, 103190.336))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (103191.95, 1.6135107, 103190.336))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1031698.7, 1.6135954, 1031697.06))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (10316953.0, 1.598131, 10316951.0))\n",
      "('Took', 142.5678198337555, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.6599076, 0.00017293543, 2.6597347))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.6575909, 3.054738e-07, 2.6575906))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.6575909, 3.054738e-07, 2.6575906))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.8990204, 0.26752684, 3.6314936))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.8988993, 0.2674427, 3.6314566))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (25.029196, 0.32594964, 24.703247))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (25.027544, 0.32569596, 24.701849))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (241.00264, 0.33308965, 240.66956))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (240.95058, 0.33306623, 240.61751))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (240.95058, 0.33306623, 240.61751))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2404.3728, 0.33352017, 2404.0393))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2404.3728, 0.33352017, 2404.0393))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (24032.307, 2.3217008, 24029.984))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (24030.607, 2.2582731, 24028.35))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (240282.9, 2.3395414, 240280.56))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (240279.88, 2.338844, 240277.53))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2402754.2, 2.367438, 2402752.0))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (24028702.0, 2.5174594, 24028700.0))\n",
      "('Took', 131.73644304275513, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.1179018, 0.09409562, 2.023806))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.1162958, 0.09382203, 2.0224738))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.1162958, 0.09382203, 2.0224738))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (7.0844784, 0.8720615, 6.212417))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (7.0836954, 0.8718068, 6.211889))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (7.0836954, 0.8718068, 6.211889))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (57.814957, 1.1045433, 56.710415))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (57.812145, 1.104154, 56.707993))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (565.927, 1.1281185, 564.7989))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (565.9269, 1.1280702, 564.7988))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5649.0234, 2.0684118, 5646.955))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (5649.0234, 2.0684118, 5646.955))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (56464.758, 2.1098447, 56462.65))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (56464.758, 2.1098447, 56462.65))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (564619.44, 2.1157281, 564617.3))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (564619.44, 2.1157281, 564617.3))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (5646165.0, 2.1454294, 5646163.0))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (56466844.0, 2.182763, 56466840.0))\n",
      "('Took', 131.0706009864807, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.7344124, 0.14081751, 2.593595))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.7219105, 0.1391059, 2.5828047))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.7219105, 0.1391059, 2.5828047))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (6.361894, 0.9052121, 5.456682))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (6.35789, 0.9050143, 5.452876))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (6.35789, 0.9050143, 5.452876))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (47.44875, 1.1021703, 46.34658))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (47.44875, 1.1021703, 46.34658))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (460.1224, 1.1288271, 458.9936))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (460.1224, 1.1288271, 458.9936))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4590.717, 1.7505035, 4588.9663))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (4589.909, 1.7344038, 4588.175))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (4589.909, 1.7344038, 4588.175))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (45874.12, 1.830862, 45872.29))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (45874.12, 1.830862, 45872.29))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (458713.88, 1.8428826, 458712.03))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (458713.78, 1.8428633, 458711.94))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4587110.0, 1.9290094, 4587108.0))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (45871060.0, 1.9373659, 45871060.0))\n",
      "('Took', 144.74621891975403, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.7052916, 0.063492864, 1.6417987))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.6927098, 0.061267916, 1.6314418))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.6927098, 0.061267916, 1.6314418))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (3.6048346, 0.50308496, 3.1017497))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (3.6000051, 0.5028633, 3.097142))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (3.6000051, 0.5028633, 3.097142))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (25.749249, 0.568949, 25.1803))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (25.74775, 0.56871384, 25.179035))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (249.10916, 0.5802361, 248.52893))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (249.10909, 0.5802444, 248.52884))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2485.8274, 1.2624282, 2484.565))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2485.1008, 0.799007, 2484.3018))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2485.1008, 0.799007, 2484.3018))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (24836.2, 1.3599355, 24834.84))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (24836.195, 1.3599205, 24834.836))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (248342.05, 1.3749962, 248340.67))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (248342.05, 1.3749962, 248340.67))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2483392.0, 1.461676, 2483390.5))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (24845224.0, 1.4621446, 24845222.0))\n",
      "('Took', 148.4564139842987, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.2830578, 0.01092843, 1.2721294))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.2649338, 0.011511311, 1.2534225))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (1.2649338, 0.011511311, 1.2534225))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.654007, 0.20260927, 1.4513977))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (1.6539475, 0.20254745, 1.4514))\n",
      "('outer step:', 2)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (9.432354, 0.24966066, 9.1826935))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (9.36601, 0.24969703, 9.116313))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (9.36601, 0.24969703, 9.116313))\n",
      "('outer step:', 3)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (88.746994, 0.2568469, 88.49015))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (88.74649, 0.25687146, 88.48962))\n",
      "('outer step:', 4)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (883.83826, 0.26023844, 883.578))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (883.8344, 0.2576018, 883.5768))\n",
      "('outer step:', 5)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (8833.229, 2.358707, 8830.87))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (8833.229, 2.358707, 8830.87))\n",
      "('outer step:', 6)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (88306.336, 2.3648522, 88303.97))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (88306.336, 2.3648522, 88303.97))\n",
      "('outer step:', 7)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (883035.94, 2.3704896, 883033.56))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (883035.9, 2.3704755, 883033.5))\n",
      "('outer step:', 8)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (8830324.0, 2.5386329, 8830321.0))\n",
      "('Took', 153.57323217391968, 'seconds to run', 9, 'samples.')\n",
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (2.2069616, 0.100383416, 2.106578))\n",
      "('iter:', 100, 'total_loss, L_adv, L_flow:', (2.1246269, 0.09830362, 2.0263233))\n",
      "('iter:', 200, 'total_loss, L_adv, L_flow:', (2.1246269, 0.09830362, 2.0263233))\n",
      "('outer step:', 1)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (4.357934, 0.4035453, 3.9543889))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-9a8366726978>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mtimestart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0madv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0madv_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3bc6964ec641>\u001b[0m in \u001b[0;36mattack\u001b[1;34m(self, imgs, targets)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'attack for image'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattack_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3bc6964ec641>\u001b[0m in \u001b[0;36mattack_batch\u001b[1;34m(self, imgs, labs)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAX_ITERATIONS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m                 \u001b[1;31m# perform the attack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m                 l, stAdvs, scores, nimg = self.sess.run([\n\u001b[0;32m    390\u001b[0m \u001b[1;31m#                                                        self.train,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, session, feed_dict, fetches, step_callback, loss_callback, **run_kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mpacked_bounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packed_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mstep_callback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         optimizer_kwargs=self.optimizer_kwargs)\n\u001b[0m\u001b[0;32m    208\u001b[0m     var_vals = [\n\u001b[0;32m    209\u001b[0m         \u001b[0mpacked_var_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpacking_slice\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpacking_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packing_slices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.pyc\u001b[0m in \u001b[0;36m_minimize\u001b[1;34m(self, initial_val, loss_grad_func, equality_funcs, equality_grad_funcs, inequality_funcs, inequality_grad_funcs, packed_bounds, step_callback, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mminimize_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mminimize_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     message_lines = [\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 447\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    448\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/lbfgsb.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.pyc\u001b[0m in \u001b[0;36mloss_grad_func_wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss_grad_func_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m       \u001b[1;31m# SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_grad_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/opt/python/training/external_optimizer.pyc\u001b[0m in \u001b[0;36meval_func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m       augmented_fetch_vals = session.run(\n\u001b[1;32m--> 278\u001b[1;33m           augmented_fetches, feed_dict=augmented_feed_dict)\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import random \n",
    "def generate_adv_train_data(data, samples, targeted=True, start=0, inception=False):\n",
    "    \"\"\"\n",
    "    Generate the input data to the attack algorithm.\n",
    "    data: the images to attack\n",
    "    samples: number of samples to use\n",
    "    targeted: if true, construct targeted attacks, otherwise untargeted attacks\n",
    "    start: offset into data to use\n",
    "    inception: if targeted and inception, randomly sample 100 targets intead of 1000\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    # generating data from test_data inputs and then the targets\n",
    "    for i in range(samples):\n",
    "        if targeted:\n",
    "            if inception:\n",
    "                seq = random.sample(range(1,1001), 10)\n",
    "            else:\n",
    "                seq = range(data.test_labels.shape[1])\n",
    "\n",
    "            for j in seq:\n",
    "                if (j == np.argmax(data.test_labels[start+i])) and (inception == False):\n",
    "                    continue\n",
    "                inputs.append(data.test_data[start+i])\n",
    "                targets.append(np.eye(data.test_labels.shape[1])[j])\n",
    "                labels.append(data.test_labels[start+i])\n",
    "        else:\n",
    "            inputs.append(data.test_data[start+i])\n",
    "            targets.append(data.test_labels[start+i])\n",
    "            labels.append(data.test_labels[start+i])\n",
    "\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return inputs, targets, labels\n",
    "\n",
    "adv_inputs = np.array([])\n",
    "adv_targets = np.array([])\n",
    "true_labels = np.array([])\n",
    "with tf.Session() as sess:\n",
    "    i = 0\n",
    "\n",
    "    while(i<50):\n",
    "        data, model =  MNIST(), MNISTModel(\"models/mnist_baseline\", sess)\n",
    "        attack = STAdv(sess, model, batch_size=9, max_iterations=1000, confidence=0)\n",
    "\n",
    "        inputs, targets, labels = generate_adv_train_data(data, samples=1, targeted=True,\n",
    "                                        start=i, inception=False)\n",
    "        i = i+1\n",
    "        timestart = time.time()\n",
    "        adv = attack.attack(inputs, targets)\n",
    "        if(adv_inputs.shape[0]==0):\n",
    "            adv_inputs = adv\n",
    "            adv_targets = targets\n",
    "            true_labels = labels\n",
    "        else:\n",
    "            adv_inputs = np.concatenate((adv_inputs, adv))\n",
    "            adv_targets = np.concatenate((adv_targets, targets))\n",
    "            true_labels = np.concatenate((true_labels, labels))\n",
    "        timeend = time.time()\n",
    "\n",
    "        print(\"Took\",timeend-timestart,\"seconds to run\",len(inputs),\"samples.\")\n",
    "\n",
    "        image_size = 28\n",
    "    print(\"adv shape:\", adv_inputs.shape)\n",
    "    print(\"adv targets:\", adv_targets.shape)\n",
    "    print(\"adv labels:\", true_labels.shape)\n",
    "    \n",
    "    np.save(\"adv_inputs\", adv_inputs)\n",
    "    np.save(\"adv_targets\", adv_targets)\n",
    "    np.save(\"true_labels\", true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('meshgrid shape', TensorShape([Dimension(2352)]))\n",
      "('batch_tile grid shape', TensorShape([Dimension(21168)]))\n",
      "('batch+3:', TensorShape([Dimension(9), Dimension(3), Dimension(784)]))\n",
      "('grid x_s shape:', TensorShape([Dimension(9), Dimension(1), Dimension(784)]))\n",
      "('x_flatten_shape:', TensorShape([Dimension(7056)]))\n",
      "('base shape:', TensorShape([Dimension(None)]))\n",
      "('base_y0_shape', TensorShape([Dimension(7056)]))\n",
      "('indices_shape', TensorShape([Dimension(7056)]))\n",
      "(9, 28, 28, 1)\n",
      "('go up to', 9)\n",
      "('attack for image', 0)\n",
      "('outer step:', 0)\n",
      "('iter:', 0, 'total_loss, L_adv, L_flow:', (1.2053373, 0.14506906, 1.0602683))\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import random \n",
    "def generate_adv_train_data(data, samples, targeted=True, start=0, inception=False):\n",
    "    \"\"\"\n",
    "    Generate the input data to the attack algorithm.\n",
    "    data: the images to attack\n",
    "    samples: number of samples to use\n",
    "    targeted: if true, construct targeted attacks, otherwise untargeted attacks\n",
    "    start: offset into data to use\n",
    "    inception: if targeted and inception, randomly sample 100 targets intead of 1000\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    labels = []\n",
    "    # generating data from test_data inputs and then the targets\n",
    "    for i in range(samples):\n",
    "        if targeted:\n",
    "            if inception:\n",
    "                seq = random.sample(range(1,1001), 10)\n",
    "            else:\n",
    "                seq = range(data.test_labels.shape[1])\n",
    "\n",
    "            for j in seq:\n",
    "                if (j == np.argmax(data.test_labels[start+i])) and (inception == False):\n",
    "                    continue\n",
    "                inputs.append(data.test_data[start+i])\n",
    "                targets.append(np.eye(data.test_labels.shape[1])[j])\n",
    "                labels.append(data.test_labels[start+i])\n",
    "        else:\n",
    "            inputs.append(data.test_data[start+i])\n",
    "            targets.append(data.test_labels[start+i])\n",
    "            labels.append(data.test_labels[start+i])\n",
    "\n",
    "\n",
    "    inputs = np.array(inputs)\n",
    "    targets = np.array(targets)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return inputs, targets, labels\n",
    "\n",
    "adv_inputs = np.load('./adv_inputs.npy')\n",
    "adv_targets = np.load('./adv_targets.npy')\n",
    "true_labels = np.load('./true_labels.npy')\n",
    "with tf.Session() as sess:\n",
    "    i = 21\n",
    "\n",
    "    while(i<50):\n",
    "        data, model =  MNIST(), MNISTModel(\"models/mnist_baseline\", sess)\n",
    "        attack = STAdv(sess, model, batch_size=9, max_iterations=1000, confidence=0)\n",
    "\n",
    "        inputs, targets, labels = generate_adv_train_data(data, samples=1, targeted=True,\n",
    "                                        start=i, inception=False)\n",
    "        i = i+1\n",
    "        timestart = time.time()\n",
    "        adv = attack.attack(inputs, targets)\n",
    "        if(adv_inputs.shape[0]==0):\n",
    "            adv_inputs = adv\n",
    "            adv_targets = targets\n",
    "            true_labels = labels\n",
    "        else:\n",
    "            adv_inputs = np.concatenate((adv_inputs, adv))\n",
    "            adv_targets = np.concatenate((adv_targets, targets))\n",
    "            true_labels = np.concatenate((true_labels, labels))\n",
    "        timeend = time.time()\n",
    "\n",
    "        print(\"Took\",timeend-timestart,\"seconds to run\",len(inputs),\"samples.\")\n",
    "\n",
    "        image_size = 28\n",
    "    print(\"adv shape:\", adv_inputs.shape)\n",
    "    print(\"adv targets:\", adv_targets.shape)\n",
    "    print(\"adv labels:\", true_labels.shape)\n",
    "    \n",
    "    np.save(\"adv_inputs\", adv_inputs)\n",
    "    np.save(\"adv_targets\", adv_targets)\n",
    "    np.save(\"true_labels\", true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
